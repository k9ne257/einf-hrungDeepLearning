{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aufgabe 1",
   "id": "e4b84e6111537b37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vorteile von Convolutional Layers gegenüber Fully Connected Layers:\n",
    "\n",
    "Convolutional Layers verwenden Filter, die Parameter teilen, was die Gesamtzahl der zu lernenden Parameter reduziert. Fully Connected Layers benötigen für jeden Knoten im Netzwerk was bei großen Eingaben zu einem exponentiellen Anstieg der Parameter führt.\n",
    "Convolutional Layers erkennen Merkmale unabhängig von ihrer genauen Position im Eingabebild und können durch den Fokus auf lokale Regionen im Bild können Convolutional Layers spezifische Details extrahieren.\n",
    "\n",
    "Sie berücksichtigen die Nachbarschaftsinformationen in Daten (z. B. Bilder), was bei Fully Connected Layers verloren geht.\n",
    "\n",
    "Sie sind weniger speicherintensiv und schneller zu trainieren im Vergleich zu Fully Connected Layers bei denselben Eingaben.\n",
    "    \n",
    "\n",
    "### Nachteile von Convolutional Layers gegenüber Fully Connected Layers:\n",
    "\n",
    "Convolutional Layers sind primär für datenräumliche Beziehungen wie Bilder und Zeitreihen nützlich, während Fully Connected Layers universeller einsetzbar sind.\n",
    "\n",
    "Komplexere Modellierung globaler Beziehungen da Convolutional Layers nur lokale Muster betrachten, benötigen sie oft mehrere Schichten, um globale Muster im gesamten Bild zu lernen.\n",
    "\n",
    "Spezialisierung durch Architektur, weil sie sind stark auf die Gestaltung der Filter und die Architekturparameter (z. B. Filtergröße, Stride) angewiesen, die für unterschiedliche Aufgaben spezifisch sind.\n"
   ],
   "id": "e216b8d982b64336"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aufgabe 2",
   "id": "6c43f7edbf7cc309"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Aufgabe 2a\n",
    "\n",
    "https://www.kaggle.com/code/georgiisirotenko/pytorch-flowers-translearing-ensemble-test-99-67\n",
    "\n",
    "Im Datenvorbereitungsschritt werden die Bilder durch verschiedene Techniken augmentiert und normalisiert, um die Trainingsdaten künstlich zu erweitern und die Robustheit des Modells zu verbessern. Konkret wird Folgendes durchgeführt:\n",
    "\n",
    "    Größenänderung  transform.Resize(): Bilder werden auf eine einheitliche Größe von 220×220220×220 Pixel skaliert.\n",
    "    Farbanpassungen  transform.ColorJitter(): ColorJitter verändert Helligkeit, Kontrast und Sättigung der Bilder.\n",
    "    Rotationen (transform.RandomRotation()) und Transformationen (transform.RandomAffine()): RandomRotation und RandomAffine führen zufällige Rotationen und affine Transformationen wie Skalierung und Verschiebung durch.\n",
    "    Horizontal Flip: Zufälliges horizontales Spiegeln der Bilder.\n",
    "    Random Erasing: Kleine Bildbereiche werden zufällig gelöscht, um Okklusionen zu simulieren.\n",
    "    Normalisierung transform.Normalize(): Die Pixelwerte werden kanalweise normalisiert (Mittelwert und Standardabweichung für jeden Kanal werden angepasst)."
   ],
   "id": "59b3ca1c6568de34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aufgabe 2b",
   "id": "7cd49b41dbc3b450"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Augmentierung hilft, die begrenzte Anzahl an Originalbildern zu erweitern, indem leicht veränderte Versionen der Originalbilder erstellt werden. Durch verbesserung der Generalisierungsfähigkeit lernt das Modell robuster gegenüber Variationen wie Beleuchtung, Perspektive oder anderen äußeren Einflüssen zu sein, die in realen Szenarien auftreten können. Die größere Vielfalt der Daten hilft, um Overfitting zu vermeiden. Transformationen wie Random Erasing oder horizontal Flip simulieren Szenarien wie teilweise verdeckte Objekte oder umgedrehte Perspektiven.",
   "id": "a8d25d2003d81a7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aufgabe 2c",
   "id": "a52d13a36745335c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Die Modelle, DenseNet121, GoogLeNet, ResNet101 & VGG19 sind vortrainiert auf großen Datensätzen (z. B. ImageNet). Dieses vortraining stellt sicher, dass sie bereits allgemeine Merkmale wie Kanten und Texturen erkennen können.\n",
    "\n",
    "Die Modelle werden kombiniert, um die Klassifikation durchzuführen. Das Ensemble-Modell summiert die Ausgaben der einzelnen Modelle und trifft auf Basis der kombinierten Ergebnisse die endgültige Entscheidung. Jedes Modell gibt Wahrscheinlichkeiten für die Klassenzugehörigkeit aus, die dann addiert werden, um die Gesamtwahrscheinlichkeit für jede Klasse zu bestimmen.\n",
    "\n",
    "zu Beginn wird nur der Klassifikator (z. B. das letzte vollständig verbundene Layer) trainiert, während die restlichen Schichten eingefroren bleiben. Nach einigen Epochen werden alle Schichten \"freigeschaltet\" (Unfreezing), sodass das gesamte Netzwerk an die spezifischen Daten angepasst wird.\n",
    "\n",
    "Die Bilder werden den Modellen als Eingabe gegeben. Die Ausgaben der Modelle werden summiert, um die wahrscheinlichste Klasse zu bestimmen. Die Bilder werden in Batches verarbeitet, um Speicher effizient zu nutzen. Parameter wie pin_memory und num_workers optimieren den Datenfluss zwischen CPU und GPU.\n",
    "\n",
    "Das Ensemble liefert die endgültige Vorhersage basierend auf den kombinierten Wahrscheinlichkeiten der Klassen."
   ],
   "id": "ef22392160b30c94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aufgabe 2d",
   "id": "9c807c5549f1311b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Im Fine-Tuning werden in der ersten Phase nur die Gewichte der Klassifikationsschicht trainiert, da alle anderen Schichten des vortrainierten Modells eingefroren sind. Die Klassifikationsschicht wird neu initialisiert, um die spezifischen Klassen des Blumen-Datensatzes zu lernen.\n",
    "\n",
    "In der zweiten Phase, nach dem \"Unfreezing\", werden alle Schichten des Netzwerks trainiert. Dadurch können die vortrainierten Gewichte an die besonderen Merkmale des Blumen-Datensatzes angepasst werden, wie spezifische Farben oder Texturen. Ein Adam-Optimierer aktualisiert die Gewichte basierend auf den Gradienten des Fehlers, und ein Scheduler passt die Lernrate dynamisch an, um die Leistung zu optimieren."
   ],
   "id": "c42f0fc8aebae908"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aufgabe 2e",
   "id": "8bc7cadea20b0a81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vollständige wissenschaftliche Quelle:\n",
    "\n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385.\n",
    "\n",
    "** hier habe ich chatgpt und google scholar verwendet "
   ],
   "id": "27f97a239f0cd5e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aufgabe 2f",
   "id": "eff22e8e55a941cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Indem mehrere Modelle kombiniert werden, kann das Ensemble die individuellen Stärken der einzelnen Modelle nutzen und Schwächen einzelner Modelle ausgleichen, um Vorhersagegenauigkeit zu erhöhen und die Robustheit des Modells zu verbessern.\n",
    "\n",
    "Der Hauptvorteil des Ensembling liegt darin, dass es die Wahrscheinlichkeit von Fehlklassifikationen reduziert und die Generalisierungsfähigkeit auf unbekannte Daten erhöht."
   ],
   "id": "3cb41a9096af0197"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aufgabe 3\n",
   "id": "11d431d8e110a774"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Feature-Extraktion aus Bildern: Extrahiere und identifiziere automatisch Merkmale aus einem Satz von Bildern, z.B. Paare von Fotos aus einem Wald, einmal vor und einmal nach einer Müllsammelaktion. Ziel ist die Zählung von gesammeltem Müll und die Erfassung, ob noch Müll im Wald liegt\n",
    "\n",
    "Zusätzliche Annahme\n",
    "    \n",
    "    Die Bilderpaare (vorher/nachher) sind zeitlich und räumlich aufeinander abgestimmt (gleicher Kamerastandort und Winkel)\n",
    "    Müll erscheint optisch deutlich sichtbar und unterscheidet sich von der natürlichen Umgebung.\n",
    "\n",
    "Art der Daten\n",
    "\n",
    "    Paare von RGB-Bildern, aufgenommen vor und nach der Müllsammelaktion.\n",
    "\n",
    "Lernmethode\n",
    "\n",
    "    Supervised Learning, da annotierte Daten benötigt werden (z. B. Segmentierung des Mülls).\n",
    "\n",
    "Neuronale Architektur\n",
    "\n",
    "    Convolutional Neural Network (CNN), z. B. ResNet, für Feature-Extraktion.\n",
    "    Für Segmentierung und genaue Lokalisierung: U-Net oder Mask R-CNN.\n",
    "\n",
    "Trainingsablauf\n",
    "\n",
    "    Jedes Bildpaar wird als Eingabe verwendet.\n",
    "    Ein CNN extrahiert Merkmale, und ein Differenzmodul berechnet pixelweise Unterschiede zwischen den Bildern.\n",
    "    Segmentierungsmodelle markieren Müllregionen im Bild.\n",
    "    Müllanzahl als Regression.\n",
    "    Klassifikation, ob noch Müll vorhanden ist (binär).\n",
    "\n",
    "Loss-Funktion\n",
    "\n",
    "    MSE zur Minimierung der Abweichung zwischen vorhergesagter und tatsächlicher Müllanzahl.\n",
    "    Binary Cross-Entropy (BCE) zur Klassifikation, ob Müll vorhanden ist.\n",
    "    \n",
    "Vortrainiertes Modell\n",
    "\n",
    "    ResNet oder EfficientNet, vortrainiert auf ImageNet, für Feature-Extraktion.\n",
    "    Mask R-CNN, vortrainiert auf COCO, für Müllsegmentierung.\n",
    "\n",
    "Erfolgsmessung\n",
    "\n",
    "    Genauigkeit der Müllzählung (z. B. mittlere absolute Abweichung).\n",
    "    F1-Score und Precision/Recall für die Klassifikation.\n",
    "    \n",
    "Schwierigkeiten im Training\n",
    "\n",
    "    Müll mit selbe Farbe wie die umgebeung könnte schwer sein zu unterscheiden (\"Kamouflage-effekt\") \n",
    "    Unterschiedliche Beleuchtung oder Kamerawinkel könnten die Differenzanalyse erschweren.\n",
    "    Annotierte Daten (z. B. Müllsegmente) könnten begrenzt sein.\n",
    "\n",
    "Schwierigkeiten in der Anwendung\n",
    "\n",
    "    Verdeckter Müll oder Ähnlichkeiten zwischen Müll und natürlichen Objekten könnten zu Fehlklassifikationen führen.\n",
    "    Unterschiedliche Umgebungen (z. B. dichte Vegetation) könnten die Generalisierungsfähigkeit beeinträchtigen."
   ],
   "id": "8bfad99b108fe259"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. Generierung von Produktbeschreibungen: Gegeben für zahlreiche Produkte je ein Datenblatt,\n",
    "Werbetexte und Produktfotos eines Herstellers, erstellen Sie neutrale vergleichbare Produkt-\n",
    "beschreibungen"
   ],
   "id": "62d57ba5fae5dcbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zusätzliche Annahmen / Spezifische Aufgabe\n",
    "\n",
    "    Es wird angenommen, dass die Daten strukturiert vorliegen (z. B. Schlüssel-Wert-Paare in den Datenblättern) und die Werbetexte gut lesbar sind.\n",
    "\n",
    "Art der Daten\n",
    "\n",
    "    Tabellarische Daten: Datenblätter mit technischen Informationen.\n",
    "    Textdaten: Werbetexte, die das Produkt beschreiben.\n",
    "    Bilddaten: Produktfotos in RGB-Format.\n",
    "\n",
    "Lernmethode\n",
    "\n",
    "    Supervised Learning, mit annotierten Beispielen für Produktbeschreibungen, die auf den gegebenen Eingabedaten basieren.\n",
    "\n",
    "Neuronale Architektur\n",
    "\n",
    "    Für tabellarische Daten kann eine Kombination aus einem Transformer-Modell und Feedforward-Schichten verwendet werden.\n",
    "\n",
    "Trainingsablauf\n",
    "\n",
    "    Eingabedatenverarbeitung:\n",
    "        Tabellarische Daten: Normalisierung und Codierung in Features.\n",
    "        Textdaten: Tokenisierung mit einem Sprachmodell.\n",
    "        Bilddaten: Feature-Extraktion mit einem vortrainierten CNN (z. B. ResNet).\n",
    "    Textgenerierung:\n",
    "        Das Seq2Seq-Modell generiert die Produktbeschreibung basierend auf den fusionierten Features.\n",
    "    Zielvariable: Die generierten Beschreibungen werden mit den referenzierten neutralen Beschreibungen verglichen.\n",
    "\n",
    "Loss-Funktion\n",
    "\n",
    "    Cross-Entropy-Loss für die Generierung, um die Abweichung zwischen vorhergesagtem und tatsächlichem Text zu minimieren.\n",
    "    Cosine Similarity Loss für die semantische Übereinstimmung zwischen generiertem und referenziellem Text.\n",
    "\n",
    "Vortrainiertes Modell\n",
    "\n",
    "    T5 oder BART als generatives Sprachmodell, vortrainiert auf einem großen Textkorpus.\n",
    "    ResNet für die Verarbeitung der Bilddaten.\n",
    "    TabTransformer für tabellarische Daten.\n",
    "\n",
    "Erfolgsmessung\n",
    "\n",
    "    ROUGE-Score, um die Ähnlichkeit zwischen generierten und referenzierten Beschreibungen zu messen.\n",
    "    Human Evaluation, um die Verständlichkeit und Neutralität zu bewerten.\n",
    "\n",
    "Schwierigkeiten im Training\n",
    "\n",
    "    Inkompatible Datenquellen (z. B. fehlende Informationen in Datenblättern oder unklare Werbetexte).\n",
    "    Das Modell könnte unnötige Details aus den Werbetexten übernehmen und die Neutralität verlieren.\n",
    "\n",
    "Schwierigkeiten in der Anwendung\n",
    "\n",
    "    Neue oder unbekannte Produkte könnten zu Fehlern führen, da das Modell auf bisher bekannten Daten basiert.\n",
    "    Uneinheitliche Datenqualität, z. B. verrauschte Produktfotos oder inkonsistente Formatierung von Datenblättern, könnten die Leistung beeinträchtigen."
   ],
   "id": "b03ac43cfc1db289"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
